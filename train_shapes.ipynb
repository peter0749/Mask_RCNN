{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(5000, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(1000, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1bf5dda58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1bee5c828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1becd67f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1bef2bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/peter/Mask_RCNN/logs/shapes20180303T2256/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "578/625 [==========================>...] - ETA: 20s - loss: 1.1974 - rpn_class_loss: 0.0161 - rpn_bbox_loss: 0.4470 - mrcnn_class_loss: 0.2250 - mrcnn_bbox_loss: 0.1860 - mrcnn_mask_loss: 0.3232"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "Invalid bounding box with area of zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/peter/anaconda3/envs/dnn/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 390, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"/home/peter/Mask_RCNN/model.py\", line 1671, in __getitem__\n    load_image_gt(self.dataset, self.config, image_id, augment=self.augment, use_mini_mask=self.config.USE_MINI_MASK, imgaug_sequence=self.imgaug_sequence)\n  File \"/home/peter/Mask_RCNN/model.py\", line 1224, in load_image_gt\n    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n  File \"/home/peter/Mask_RCNN/utils.py\", line 462, in minimize_mask\n    raise Exception(\"Invalid bounding box with area of zero\")\nException: Invalid bounding box with area of zero\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Invalid bounding box with area of zero",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m~/Mask_RCNN/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers)\u001b[0m\n\u001b[1;32m   2249\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2251\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2252\u001b[0m         )\n\u001b[1;32m   2253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2210\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: Invalid bounding box with area of zero"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=10, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
