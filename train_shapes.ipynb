{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    # STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    # VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(670, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(65, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f556915c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5568eb4ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5568e12da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5568c2f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/peter/Mask_RCNN/logs/shapes20180304T0052/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35/84 [===========>..................] - ETA: 40s - loss: 3.2678 - rpn_class_loss: 0.0454 - rpn_bbox_loss: 0.8868 - mrcnn_class_loss: 0.7997 - mrcnn_bbox_loss: 0.7202 - mrcnn_mask_loss: 0.8157"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=30, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
