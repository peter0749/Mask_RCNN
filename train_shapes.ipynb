{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    # STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    # VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(670, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(65, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf8f6a87f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf8f3d5f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf8f35f5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf8f1508d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/peter/Mask_RCNN/logs/shapes20180304T0214/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "84/84 [==============================] - 60s 710ms/step - loss: 1.9832 - rpn_class_loss: 0.0285 - rpn_bbox_loss: 0.6151 - mrcnn_class_loss: 0.4905 - mrcnn_bbox_loss: 0.4588 - mrcnn_mask_loss: 0.3902 - val_loss: 1.3454 - val_rpn_class_loss: 0.0182 - val_rpn_bbox_loss: 0.4493 - val_mrcnn_class_loss: 0.2487 - val_mrcnn_bbox_loss: 0.2823 - val_mrcnn_mask_loss: 0.3470\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=30, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
