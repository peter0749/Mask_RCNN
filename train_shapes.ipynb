{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(60000, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(10000, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ab373c048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa50beba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa4f92f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa4df09e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/peter/Mask_RCNN/logs/shapes20180303T2217/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/dnn/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 780/7500 [==>...........................] - ETA: 43:58 - loss: 0.9936 - rpn_class_loss: 0.0145 - rpn_bbox_loss: 0.4301 - mrcnn_class_loss: 0.1664 - mrcnn_bbox_loss: 0.1396 - mrcnn_mask_loss: 0.2430"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=10, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
